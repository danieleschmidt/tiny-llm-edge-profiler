name: Continuous Integration

on:
  push:
    branches: [ main, develop, 'feature/*', 'release/*', 'hotfix/*' ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  MOCK_HARDWARE: 'true'
  CI: 'true'

jobs:
  # =============================================================================
  # Code Quality and Security
  # =============================================================================
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Run code formatting check
      run: |
        black --check src/ tests/ scripts/
        isort --check-only src/ tests/ scripts/
    
    - name: Run linting
      run: |
        flake8 src/ tests/ scripts/
        pylint src/ --output-format=json > pylint-report.json || true
    
    - name: Run type checking
      run: mypy src/ tests/
    
    - name: Run security checks
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        safety check --json --output safety-report.json || true
        pip-audit --format=json --output=pip-audit-report.json || true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
          pip-audit-report.json
          pylint-report.json

  # =============================================================================
  # Testing Matrix
  # =============================================================================
  test:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']
        exclude:
          # Reduce matrix size for faster CI
          - os: windows-latest
            python-version: '3.8'
          - os: windows-latest
            python-version: '3.9'
          - os: macos-latest
            python-version: '3.8'
          - os: macos-latest
            python-version: '3.9'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install system dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y libusb-1.0-0-dev libudev-dev
    
    - name: Install system dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install libusb
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --junit-xml=pytest-report.xml
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v \
          --junit-xml=pytest-integration-report.xml
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          pytest-report.xml
          pytest-integration-report.xml
          htmlcov/
          coverage.xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # =============================================================================
  # Hardware Testing (when available)
  # =============================================================================
  hardware-test:
    name: Hardware Tests
    runs-on: self-hosted  # Requires self-hosted runner with hardware
    timeout-minutes: 45
    if: github.event_name == 'schedule' || contains(github.event.pull_request.labels.*.name, 'hardware-test')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[hardware]"
    
    - name: Check hardware availability
      run: |
        ls -la /dev/tty* | grep -E "(USB|ACM)" || echo "No hardware devices found"
        lsusb | grep -E "(10c4|0483|2e8a|1366)" || echo "No known dev boards found"
    
    - name: Run hardware tests
      env:
        MOCK_HARDWARE: 'false'
      run: |
        pytest tests/hardware/ -v \
          --hardware \
          --junit-xml=pytest-hardware-report.xml || true
    
    - name: Upload hardware test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: hardware-test-results
        path: pytest-hardware-report.xml

  # =============================================================================
  # Documentation Build
  # =============================================================================
  docs:
    name: Documentation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[docs]"
    
    - name: Build documentation
      run: |
        cd docs
        make html
    
    - name: Check documentation links
      run: |
        cd docs
        make linkcheck || true
    
    - name: Upload documentation
      uses: actions/upload-artifact@v3
      with:
        name: documentation
        path: docs/_build/html/

  # =============================================================================
  # Performance Benchmarks
  # =============================================================================
  benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark
        pip install -e .
    
    - name: Run benchmarks
      run: |
        pytest tests/ --benchmark-only \
          --benchmark-json=benchmark-results.json
    
    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true

  # =============================================================================
  # Container Testing
  # =============================================================================
  docker:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build development image
      uses: docker/build-push-action@v5
      with:
        context: .
        target: development
        tags: tiny-llm-profiler:dev
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Build production image
      uses: docker/build-push-action@v5
      with:
        context: .
        target: production
        tags: tiny-llm-profiler:prod
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Test container functionality
      run: |
        docker run --rm tiny-llm-profiler:prod tiny-profiler --version
        docker run --rm tiny-llm-profiler:prod python -c "import tiny_llm_profiler; print('OK')"
    
    - name: Run security scan on image
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'tiny-llm-profiler:prod'
        format: 'json'
        output: 'trivy-results.json'
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: trivy-results
        path: trivy-results.json

  # =============================================================================
  # Dependency Vulnerability Scanning
  # =============================================================================
  dependency-scan:
    name: Dependency Vulnerability Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Run Snyk to check for vulnerabilities
      uses: snyk/actions/python@master
      env:
        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
      with:
        args: --severity-threshold=high --json-file-output=snyk-results.json
      continue-on-error: true
    
    - name: Upload Snyk results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: snyk-results
        path: snyk-results.json

  # =============================================================================
  # Build Artifacts
  # =============================================================================
  build:
    name: Build Distribution
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [code-quality, test]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for version calculation
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine
    
    - name: Build package
      run: python -m build
    
    - name: Check package
      run: twine check dist/*
    
    - name: Upload distribution artifacts
      uses: actions/upload-artifact@v3
      with:
        name: python-package-distributions
        path: dist/

  # =============================================================================
  # Status Reporting
  # =============================================================================
  status:
    name: CI Status
    runs-on: ubuntu-latest
    if: always()
    needs: [code-quality, test, docs, docker, build]
    
    steps:
    - name: Check CI Status
      run: |
        echo "Code Quality: ${{ needs.code-quality.result }}"
        echo "Tests: ${{ needs.test.result }}"
        echo "Documentation: ${{ needs.docs.result }}"
        echo "Docker: ${{ needs.docker.result }}"
        echo "Build: ${{ needs.build.result }}"
        
        if [[ "${{ needs.code-quality.result }}" == "failure" || 
              "${{ needs.test.result }}" == "failure" || 
              "${{ needs.docs.result }}" == "failure" || 
              "${{ needs.docker.result }}" == "failure" || 
              "${{ needs.build.result }}" == "failure" ]]; then
          echo "❌ CI Pipeline Failed"
          exit 1
        else
          echo "✅ CI Pipeline Passed"
        fi